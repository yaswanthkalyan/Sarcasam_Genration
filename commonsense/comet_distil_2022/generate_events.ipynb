{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grave-scholar",
   "metadata": {},
   "source": [
    "# Initialize Generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Set parameters for generation of events\n",
    "\n",
    "Run this block only once, as it overwrite ``batches'' and\n",
    "    ``anon_heads'' which keep track of generations\n",
    "\n",
    "'''\n",
    "import openai\n",
    "\n",
    "\n",
    "# this file should contain openai key for generation\n",
    "#  GPT-3 access is REQUIRED for this script\n",
    "#  (although GPT-3 calls could be replaced with calls\n",
    "#  to a local language model--this would require code changes)\n",
    "with open('api.key') as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "\n",
    "engine = 'davinci' # gpt-3 model version to use\n",
    "length= 12 # maximum token length of generations. 12 tokens is up to 12 words, but likely less\n",
    "top_p = 0.9 # top_p to use for nucleus sampling\n",
    "n_gen = 100 # number of generations per-batch\n",
    "presence_penalty = 0.5 # discourage tokens in the prompt\n",
    "frequency_penalty = 0.5 # discourage tokens in the prompt based on frequency\n",
    "PersonX = 'PersonX' # name to use for PersonX. Default is just \"PersonX\"\n",
    "PersonY = 'PersonY' # name to use for PersonY. Default is just \"PersonY\"\n",
    "\n",
    "\n",
    "\n",
    "n_examples_in = 10# how many few-shot examples to use per generation batch\n",
    "n_batches = 2 # how many generation batches to run\n",
    "\n",
    "\n",
    "\n",
    "batches = []\n",
    "\n",
    "anon_heads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-france",
   "metadata": {},
   "source": [
    "# Definitions for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Load exampe events for few-shot generation, and define\n",
    "function to convert these into few-shot prompts\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "with open('seed_events.txt') as f:\n",
    "    seed_events = [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "## converts an example into text for few-shot\n",
    "def ex2text(ex, include_gen = True, number = None):\n",
    "    text = ''\n",
    "    if number is not None:\n",
    "        text += '{}. Event:'.format(number)\n",
    "        \n",
    "    text += ''.format()\n",
    "\n",
    "    if include_gen:\n",
    "        text += ' {}\\n\\n\\n'.format(ex)\n",
    "    return text\n",
    "\n",
    "\n",
    "## create a few-shot prompt with the final \n",
    "## example left open for generation\n",
    "##\n",
    "### note: ex2text should be a function that takes \n",
    "## an example from examples and produces a string \n",
    "## template. This should accept an argument, include_gen \n",
    "## which is a bool to decide whether to leave it open for\n",
    "## generation or include the gt\n",
    "def few_shot_prompt(examples, ex2text, number = None, Person_list = None):\n",
    "    template_str = ''\n",
    "    \n",
    "    i = -1\n",
    "    for i, example in enumerate(examples[:-1]):\n",
    "        \n",
    "        if number:\n",
    "            ex_str = ex2text(example, include_gen = True, number = i + 1)\n",
    "        else:\n",
    "            ex_str = ex2text(example, include_gen = True)\n",
    "            \n",
    "            \n",
    "        if Person_list is not None:\n",
    "            ex_str = name_PX_PY(ex_str, Person_list[i][0],Person_list[i][1] )\n",
    "            \n",
    "        template_str += ex_str\n",
    "        \n",
    "    i = i + 1\n",
    "    \n",
    "    if number:\n",
    "        ex_str = ex2text(examples[-1], include_gen = False,number = i + 1)\n",
    "    else:\n",
    "        ex_str = ex2text(examples[-1], include_gen = False)\n",
    "        \n",
    "    if Person_list is not None:\n",
    "        ex_str = name_PX_PY(ex_str, Person_list[i][0],Person_list[i][1] )\n",
    "    \n",
    "    template_str += ex_str\n",
    "    \n",
    "    return template_str\n",
    "\n",
    "\n",
    "def scrub_PX_PY(s, PersonX, PersonY):\n",
    "    return s.replace(PersonX, 'PersonX').replace(PersonY, 'PersonY')\n",
    "\n",
    "def name_PX_PY(s, PersonX, PersonY):\n",
    "    return s.replace('PersonX', PersonX).replace('PersonY', PersonY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-occupation",
   "metadata": {},
   "source": [
    "# Generate Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "from event_utils import few_shot_prompt, name_PX_PY, scrub_PX_PY, complete_gpt3\n",
    "\n",
    "for _ in range(n_batches):\n",
    "    # sleep to prevent timout from api\n",
    "    time.sleep(0.05)\n",
    "    # randomize which events are used for generation\n",
    "    random.shuffle(seed_events)\n",
    "    examples_in = seed_events[:n_examples_in]\n",
    "\n",
    "    # outputs for this batch\n",
    "    outputs = []\n",
    "\n",
    "    # use names defined abov\n",
    "    names = (PersonX,PersonY)\n",
    "    # define prompt based on examples and names\n",
    "    prompt = few_shot_prompt(examples_in + [''], ex2text, number=True)\n",
    "    prompt = name_PX_PY(prompt,names[0],names[1])\n",
    "\n",
    "\n",
    "    ## generate using the prompt\n",
    "    print('='*20 + 'prompt'+ '='*20)\n",
    "    print(prompt)\n",
    "    result = complete_gpt3(prompt, length, engine, top_p = top_p,num_log_probs=1,n=n_gen, stop='\\n\\n', echo=False,\n",
    "                          frequency_penalty=frequency_penalty, presence_penalty=presence_penalty)\n",
    "\n",
    "\n",
    "\n",
    "    ### sort the output\n",
    "    outputs = []\n",
    "    for choice in result['choices']:\n",
    "        try:\n",
    "            out = choice['text']\n",
    "        except:\n",
    "            out = ''\n",
    "        end_ind = choice['logprobs']['text_offset'].index( max(choice['logprobs']['text_offset']))\n",
    "        nll = sum(choice['logprobs']['token_logprobs'][:end_ind + 1])\n",
    "\n",
    "        text = choice['text']\n",
    "\n",
    "        anon_text= scrub_PX_PY(out, names[0],names[1]).strip()\n",
    "\n",
    "        print('='*20 + 'out'+ '='*20)\n",
    "        print(anon_text)\n",
    "\n",
    "        ind_newline_0 = 0 \n",
    "        try:\n",
    "            ind_newline_1 = ind_newline_0 + text[ind_newline_0 + 1:].index('\\n') + 1\n",
    "        except:\n",
    "            ind_newline_1 = len(text)-1\n",
    "        #nll_explanation = get_subspan_CE(ind_newline_0 + 1, ind_newline_1, choice)\n",
    "\n",
    "        print('nll: {}'.format(nll))\n",
    "\n",
    "        ## if did not reach a stop word, don't take this generation\n",
    "        if choice['finish_reason'] != 'stop' or not (anon_text.startswith('PersonX')):\n",
    "            print('break')\n",
    "            continue\n",
    "\n",
    "        outputs  += [{'text':text,\n",
    "                      'anon_text': anon_text,\n",
    "                                             'result':choice,\n",
    "                                              'nll':nll,\n",
    "                                              'prompt':prompt}]\n",
    "    \n",
    "    \n",
    "    batches.append({'prompt':prompt, 'events':outputs})\n",
    "\n",
    "    anon_heads += [v['anon_text'] for v in outputs]\n",
    "    \n",
    "    print('{} unique new events'.format(len(set(anon_heads))))\n",
    "    \n",
    "with open('generated_events.jsonl','w') as f:\n",
    "    for batch in batches:\n",
    "        f.write(json.dumps(batch) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-center",
   "metadata": {},
   "source": [
    "# Process Generated Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "First, we load the generated head events, and sort them by negative log-likelihood (nll)\n",
    "\n",
    "We truncate the bottom 20% which are most likely to be degenerate\n",
    "\n",
    "'''\n",
    "\n",
    "## first, load the generated heads from earlier, in batches\n",
    "generated_batches = []\n",
    "with open('generated_events.jsonl','r') as f:\n",
    "    for line in f.readlines():\n",
    "        generated_batches.append(json.loads(line))\n",
    "     \n",
    "# next, get each individual event\n",
    "all_events = []\n",
    "for batch in generated_batches:\n",
    "    for d in batch['events']:\n",
    "        all_events += [(d['anon_text'],d['nll'])]\n",
    " \n",
    "\n",
    "# trim the bottom 20% in terms of nll\n",
    "all_events = {v[0]:v[1] for v in all_events}\n",
    "all_events = [(key, all_events[key]) for key in all_events.keys()]\n",
    "all_events.sort(key = lambda v: -v[1])\n",
    "all_events = all_events[:int(0.8*len(all_events))]\n",
    "\n",
    "all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Finally, remove any events with strange formatting or degenerate properties\n",
    "\n",
    "''' \n",
    "   \n",
    "todo_events = []\n",
    "\n",
    "\n",
    "for event,_ in all_events:\n",
    "\n",
    "    if any(not (c.isalnum() or c in '\\',\".-â€™$ ') for c in event) or (len(event) < len(PersonX)):\n",
    "        continue\n",
    "    todo_events += [event]\n",
    "    \n",
    "\n",
    "# write these to a file for use in inference generation\n",
    "with open('todo_events.txt','w') as f:\n",
    "    for event in todo_events:\n",
    "        f.write('{}\\n'.format(event))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-titanium",
   "metadata": {},
   "source": [
    "# [If using data release instead of generating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "!!!\n",
    "Only run this block if you are planning to use our released head events,\n",
    "rather than generating your own. This block will produce the same format of \n",
    "'todo_events.txt' file as above, but will use those generated as part of \n",
    "the Symbolic Knowledge Decoding paper, rather than generating new ones\n",
    "\n",
    "Remove the assert(False) line. We include this to prevent users from accidentally\n",
    "running this block and overwriting their own generations\n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "assert(False)\n",
    "\n",
    "events = []\n",
    "with open('downloaded/ATOMIC10X.jsonl') as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        events.append(d['head'])\n",
    "        \n",
    "\n",
    "# remove duplicates\n",
    "events = list(set(events))\n",
    "\n",
    "# write these to a file for use in inference generation\n",
    "with open('todo_events.txt','w') as f:\n",
    "    for event in events:\n",
    "        f.write('{}\\n'.format(event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-marina",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
